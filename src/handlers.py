"""
æ–‡ä»¶å¤„ç†æ¨¡å—

åŒ…å«å•æ–‡ä»¶å¤„ç†å’Œæ‰¹é‡å¤„ç†çš„é€»è¾‘
"""

import time
import traceback
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple

from torch.utils.data import DataLoader

logger = logging.getLogger(__name__)

from config import DeployConfig, DEFAULT_CONFIG
from .brep_and_graph import load_single_graph, BREPGraphDataset
from .common import data_collate
from ui.components import (
    create_empty_prediction_html,
    create_empty_confidence_html,
    create_empty_probs_html,
    format_prediction_result,
    format_batch_results,
    create_progress_html,
    create_empty_progress_html
)


class FileHandler:
    """æ–‡ä»¶å¤„ç†å™¨"""
    
    def __init__(
        self, 
        config: DeployConfig,
        classifier=None,
        is_ready: bool = False
    ):
        """
        åˆå§‹åŒ–å¤„ç†å™¨
        
        Args:
            config: éƒ¨ç½²é…ç½®
            classifier: åˆ†ç±»å™¨å®ä¾‹
            is_ready: åˆ†ç±»å™¨æ˜¯å¦å°±ç»ª
        """
        self.config = config
        self.classifier = classifier
        self.is_ready = is_ready
    
    def update_classifier(self, classifier, is_ready: bool):
        """æ›´æ–°åˆ†ç±»å™¨çŠ¶æ€"""
        self.classifier = classifier
        self.is_ready = is_ready
    
    def process_single_file(
        self, 
        file_obj
    ) -> Tuple[str, str, str, str]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶
        
        Args:
            file_obj: Gradioæ–‡ä»¶å¯¹è±¡
            
        Returns:
            (class_html, confidence_html, probs_html, viewer_html)
        """
        from ui.viewer3d import create_empty_step_viewer
        
        if file_obj is None:
            return (
                create_empty_prediction_html(),
                create_empty_confidence_html(),
                create_empty_probs_html(),
                create_empty_step_viewer()
            )
        
        start_time = time.time()
        
        try:
            file_path = file_obj.name if hasattr(file_obj, 'name') else str(file_obj)
            print(f"ğŸ“‚ å¤„ç†æ–‡ä»¶: {file_path}")
            
            # æ­¥éª¤1: åŠ è½½/æ„å»ºå›¾
            graph, metadata = load_single_graph(file_path)
            
            if graph is None:
                error_msg = metadata.get("error", "æœªçŸ¥é”™è¯¯")
                raise RuntimeError(f"å›¾æ„å»ºå¤±è´¥: {error_msg}")
            
            # æ­¥éª¤2: åˆ†ç±»é¢„æµ‹
            if self.classifier is not None and self.is_ready:
                result = self.classifier.predict(graph=graph)
                
                class_html, confidence_html, probs_html = format_prediction_result(
                    predicted_class=result["predicted_class"],
                    confidence=result["confidence"],
                    probabilities=result["probabilities"],
                    inference_time=result["inference_time"]
                )
            else:
                # æ¼”ç¤ºæ¨¡å¼
                demo_result = self._generate_demo_prediction()
                class_html, confidence_html, probs_html = format_prediction_result(
                    **demo_result
                )
            
            # ç”Ÿæˆ3DæŸ¥çœ‹å™¨
            from ui.viewer3d import create_step_viewer_html
            viewer_html = create_step_viewer_html(file_path)
            
            total_time = time.time() - start_time
            print(f"âœ“ å¤„ç†å®Œæˆï¼Œè€—æ—¶: {total_time*1000:.1f}ms")
            
            return class_html, confidence_html, probs_html, viewer_html
            
        except Exception as e:
            error_msg = str(e)
            traceback.print_exc()
            
            error_html = f"""
            <div style="text-align: center; padding: 2rem; color: #ff4d4f;">
                <svg width="48" height="48" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" style="margin: 0 auto;">
                    <circle cx="12" cy="12" r="10"/>
                    <line x1="15" y1="9" x2="9" y2="15"/>
                    <line x1="9" y1="9" x2="15" y2="15"/>
                </svg>
                <p style="margin-top: 1rem;">å¤„ç†å¤±è´¥</p>
                <p style="color: #8b949e; font-size: 0.9rem; margin-top: 0.5rem;">{error_msg}</p>
            </div>
            """
            
            from ui.viewer3d import create_empty_step_viewer
            return (
                error_html,
                create_empty_confidence_html(),
                create_empty_probs_html(),
                create_empty_step_viewer()
            )
    
    def process_batch_files(
        self, 
        file_objs: List
    ):
        """
        æ‰¹é‡å¤„ç†æ–‡ä»¶ï¼ˆä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½ + DataLoaderæ‰¹é‡æ¨ç†ï¼‰
        ä½¿ç”¨ç”Ÿæˆå™¨é€æ­¥æ›´æ–°è¿›åº¦
        
        Args:
            file_objs: Gradioæ–‡ä»¶å¯¹è±¡åˆ—è¡¨
            
        Yields:
            (è¿›åº¦HTML, è¡¨æ ¼æ•°æ®) - é€æ­¥æ›´æ–°è¿›åº¦
        """
        if not file_objs:
            yield create_empty_progress_html(), []
            return
        
        # ä»é…ç½®è¯»å–æ‰¹æ¬¡å¤§å°
        batch_size = self.config.model.batch_size
        
        # æå–æ–‡ä»¶è·¯å¾„
        file_paths = [
            file_obj.name if hasattr(file_obj, 'name') else str(file_obj)
            for file_obj in file_objs
        ]
        
        total_files = len(file_paths)
        start_time = time.time()
        
        # é˜¶æ®µ1: å›¾æ•°æ®æ„å»º
        # æ›´æ–°è¿›åº¦ï¼šå¼€å§‹åŠ è½½
        yield create_progress_html(
            stage1_progress=0,
            stage1_text=f"å¼€å§‹æ„å»ºå›¾æ•°æ®ï¼Œå…± {total_files} ä¸ªæ–‡ä»¶...",
            stage2_progress=0,
            stage2_text="ç­‰å¾…å›¾æ•°æ®æ„å»ºå®Œæˆ..."
        ), []
        
        # ä½¿ç”¨çœŸå®çš„è¿›åº¦å›è°ƒ
        progress_queue = []
        current_progress = [0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨å›è°ƒä¸­ä¿®æ”¹
        
        def progress_callback(current: int, total: int, message: str):
            """è¿›åº¦å›è°ƒå‡½æ•°"""
            if total > 0:
                progress_pct = int((current / total) * 100)
                current_progress[0] = progress_pct
                progress_queue.append((progress_pct, message))
        
        # ä½¿ç”¨ BREPGraphDataset å¤šè¿›ç¨‹æ‰¹é‡åŠ è½½å›¾
        try:
            # åœ¨åå°çº¿ç¨‹ä¸­åŠ è½½ï¼Œä¸»çº¿ç¨‹ç›‘æ§è¿›åº¦é˜Ÿåˆ—
            dataset = None
            load_error = None
            
            def load_in_background():
                nonlocal dataset, load_error
                try:
                    dataset = BREPGraphDataset(
                        file_paths=file_paths, 
                        max_workers=4,
                        progress_callback=progress_callback
                    )
                except Exception as e:
                    load_error = e
            
            import threading
            load_thread = threading.Thread(target=load_in_background)
            load_thread.start()
            
            # ç›‘æ§è¿›åº¦é˜Ÿåˆ—å¹¶æ›´æ–°UI
            while load_thread.is_alive():
                # æ£€æŸ¥è¿›åº¦é˜Ÿåˆ—
                while progress_queue:
                    progress_pct, message = progress_queue.pop(0)
                    yield create_progress_html(
                        stage1_progress=progress_pct,
                        stage1_text=message,
                        stage2_progress=0,
                        stage2_text="ç­‰å¾…å›¾æ•°æ®æ„å»ºå®Œæˆ..."
                    ), []
                
                # å¦‚æœæ²¡æœ‰æ–°è¿›åº¦ï¼Œä¹Ÿå®šæœŸæ›´æ–°å½“å‰è¿›åº¦
                import time as time_module
                time_module.sleep(0.3)  # æ¯0.3ç§’æ£€æŸ¥ä¸€æ¬¡
            
            # å¤„ç†å‰©ä½™çš„è¿›åº¦æ›´æ–°
            while progress_queue:
                progress_pct, message = progress_queue.pop(0)
                yield create_progress_html(
                    stage1_progress=progress_pct,
                    stage1_text=message,
                    stage2_progress=0,
                    stage2_text="ç­‰å¾…å›¾æ•°æ®æ„å»ºå®Œæˆ..."
                ), []
            
            load_thread.join()
            
            if load_error:
                raise load_error
            
            # æ›´æ–°è¿›åº¦ï¼šåŠ è½½å®Œæˆ
            loaded_count = len(dataset)
            stage1_text = f"å®Œæˆï¼æˆåŠŸåŠ è½½ {loaded_count}/{total_files} ä¸ªå›¾"
            yield create_progress_html(
                stage1_progress=100,
                stage1_text=stage1_text,
                stage2_progress=0,
                stage2_text="å‡†å¤‡å¼€å§‹æ¨ç†..."
            ), []
            
        except Exception as e:
            print(f"âš  æ‰¹é‡åŠ è½½å¤±è´¥: {e}")
            error_html = create_progress_html(
                stage1_progress=0,
                stage1_text=f"åŠ è½½å¤±è´¥: {str(e)}",
                stage2_progress=0,
                stage2_text="æœªå¼€å§‹"
            )
            yield error_html, []
            return
        
        if len(dataset) == 0:
            print("âš  æ²¡æœ‰æˆåŠŸåŠ è½½çš„å›¾")
            error_html = create_progress_html(
                stage1_progress=0,
                stage1_text="æ²¡æœ‰æˆåŠŸåŠ è½½çš„å›¾",
                stage2_progress=0,
                stage2_text="æœªå¼€å§‹"
            )
            yield error_html, []
            return
        
        load_time = time.time() - start_time
        print(f"âœ“ å›¾åŠ è½½å®Œæˆï¼Œè€—æ—¶: {load_time:.2f}sï¼Œå…± {len(dataset)} ä¸ªå›¾")
        
        # åˆ›å»º DataLoader
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=data_collate
        )
        
        # é˜¶æ®µ2: æ¨¡å‹æ¨ç†å¤„ç†
        total_batches = len(dataloader)
        results = []
        processed_count = 0
        
        for batch_idx, batch in enumerate(dataloader):
            batched_graph = batch["graph"]
            file_names_batch = batch.get("file_name", [])
            
            # ç¡®ä¿ file_names_batch æ˜¯åˆ—è¡¨
            if not isinstance(file_names_batch, list):
                file_names_batch = [file_names_batch]
            
            # æ›´æ–°é˜¶æ®µ2è¿›åº¦
            stage2_progress = (batch_idx + 1) / total_batches * 100 if total_batches > 0 else 0
            processed_count += len(file_names_batch)
            stage2_text = f"æ­£åœ¨å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{total_batches} (å·²å¤„ç† {processed_count} ä¸ªæ–‡ä»¶)"
            
            # é€æ­¥æ›´æ–°è¿›åº¦
            yield create_progress_html(
                stage1_progress=100,
                stage1_text=stage1_text,
                stage2_progress=stage2_progress,
                stage2_text=stage2_text
            ), format_batch_results(results) if results else []
            
            try:
                if self.classifier is not None and self.is_ready:
                    # æ‰¹é‡æ¨ç†
                    batch_results = self.classifier.predict_batch(batched_graph)
                    
                    for i, result in enumerate(batch_results):
                        # ç›´æ¥ä½¿ç”¨ file_nameï¼ˆæ ·æœ¬åï¼‰
                        filename = file_names_batch[i] if i < len(file_names_batch) else f"unknown_{i}"
                        result["filename"] = filename
                        result["status"] = "success"
                        results.append(result)
                else:
                    # æ¼”ç¤ºæ¨¡å¼
                    for i, file_name in enumerate(file_names_batch):
                        demo_result = self._generate_demo_prediction()
                        filename = file_name if file_name else f"unknown_{i}"
                        demo_result["filename"] = filename
                        demo_result["status"] = "success"
                        results.append(demo_result)
                        
            except Exception as e:
                print(f"âš  æ‰¹æ¬¡æ¨ç†å¤±è´¥: {e}")
                for i, file_name in enumerate(file_names_batch):
                    filename = file_name if file_name else f"unknown_{i}"
                    results.append({
                        "filename": filename,
                        "predicted_class": "-",
                        "confidence": 0,
                        "status": "error",
                        "inference_time": 0
                    })
        
        total_time = time.time() - start_time
        print(f"âœ“ æ‰¹é‡å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}s")
        
        # æœ€ç»ˆè¿›åº¦
        final_html = create_progress_html(
            stage1_progress=100,
            stage1_text=stage1_text,
            stage2_progress=100,
            stage2_text=f"å®Œæˆï¼å…±å¤„ç† {len(results)} ä¸ªæ–‡ä»¶ï¼Œæ€»è€—æ—¶ {total_time:.1f}s"
        )
        
        yield final_html, format_batch_results(results)
    
    def _generate_demo_prediction(self) -> Dict:
        """ç”Ÿæˆæ¼”ç¤ºé¢„æµ‹ç»“æœ"""
        import random
        
        all_classes = self.config.class_mapping.get_all_class_names()
        if not all_classes:
            all_classes = ["æ•´ä½“å¼-èºçª", "é“¸é€ å¼-è½¦å‰Š", "ç¯å½¢å¼-æ— è½´"]
        
        predicted_class = random.choice(all_classes)
        confidence = random.uniform(0.7, 0.98)
        
        probabilities = {}
        remaining = 1.0 - confidence
        
        for cls in all_classes:
            if cls == predicted_class:
                probabilities[cls] = confidence
            else:
                prob = random.uniform(0, remaining / len(all_classes))
                probabilities[cls] = prob
                remaining -= prob
        
        return {
            "predicted_class": predicted_class,
            "confidence": confidence,
            "probabilities": probabilities,
            "inference_time": 0.05
        }

