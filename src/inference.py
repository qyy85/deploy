"""
PyTorchæ¨ç†å¼•æ“

ä½¿ç”¨PyTorchè¿›è¡Œæ¨¡å‹æ¨ç†ï¼Œæ”¯æŒDGLå›¾ç¥ç»ç½‘ç»œã€‚
æ”¯æŒä¸¤ç§åŠ è½½æ¨¡å¼ï¼š
- native: åŸç”Ÿ PyTorch æ¨¡å¼ï¼Œä½¿ç”¨ state_dict æ–¹å¼åŠ è½½ï¼ˆæ¨èï¼‰
  - ä» checkpoint ä¸­åŠ è½½ encoder_config, encoder_state_dict, classifier_config, classifier_state_dict
  - é‡å»ºæ¨¡å‹ç»“æ„å¹¶åŠ è½½æƒé‡
- jit: TorchScript æ¨¡å¼ï¼Œä½¿ç”¨ torch.jit.load åŠ è½½
"""

import sys
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
import dgl

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½æ‰¾åˆ° src.models ç­‰æ¨¡å—
project_root = Path(__file__).parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# å¯¼å…¥æ¨¡å‹ç±»
from .wrapper import ClassifierWrapper
from .models import DGI, ClassifyNet


class ModelInference:
    """
    PyTorchæ¨¡å‹æ¨ç†å¼•æ“
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    - native: ä½¿ç”¨ state_dict æ–¹å¼åŠ è½½ï¼ˆæ¨èï¼‰
      - ä» checkpoint ä¸­é‡å»º encoder å’Œ classifier
      - æ›´å®‰å…¨ã€æ›´çµæ´»ï¼Œä¸ä¾èµ–å®Œæ•´çš„ç±»å®šä¹‰
    - jit: ä½¿ç”¨ TorchScript æ¨¡å¼åŠ è½½
    """
    
    def __init__(
        self,
        model_path: str,
        class_mapping: Optional[Dict[int, str]] = None,
        device: str = "cpu",
        mode: str = "native"
    ):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            model_path: å¯¼å‡ºçš„æ¨¡å‹è·¯å¾„ (.pt)
            class_mapping: ç±»åˆ«IDåˆ°åç§°çš„æ˜ å°„
            device: æ¨ç†è®¾å¤‡ ("cpu" æˆ– "cuda")
            mode: åŠ è½½æ¨¡å¼ "native" æˆ– "jit"
        """
        self.device = torch.device(device)
        self.class_mapping = class_mapping or {}
        self.model = None
        self.mode = mode
        self.num_classes = len(class_mapping) if class_mapping else None
        
        # åŠ è½½æ¨¡å‹
        self._load_model(model_path)
    
    def _load_model(self, model_path: str):
        """åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ state_dict æ–¹å¼ï¼‰"""
        model_path = Path(model_path)
        
        if not model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
        print(f"   æ¨¡å¼: {self.mode}")
        
        try:
            if self.mode == "native":
                # åŸç”Ÿæ¨¡å¼ï¼šä½¿ç”¨ state_dict æ–¹å¼åŠ è½½
                # åŠ è½½æ£€æŸ¥ç‚¹
                checkpoint = torch.load(str(model_path), map_location=self.device)
                
                # æ£€æŸ¥ checkpoint æ ¼å¼
                if isinstance(checkpoint, dict) and 'encoder_config' in checkpoint and 'classifier_config' in checkpoint:
                    # æ–°æ ¼å¼ï¼šåŒ…å«é…ç½®å’Œ state_dict
                    # é‡å»ºç¼–ç å™¨
                    encoder = DGI(**checkpoint['encoder_config'])
                    encoder.load_state_dict(checkpoint['encoder_state_dict'])
                    
                    # é‡å»ºåˆ†ç±»å™¨
                    classifier = ClassifyNet(**checkpoint['classifier_config'])
                    classifier.load_state_dict(checkpoint['classifier_state_dict'])
                    
                    # ç»„åˆæˆåŒ…è£…å™¨
                    self.model = ClassifierWrapper(encoder, classifier)
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼ã€‚æœŸæœ›åŒ…å« 'encoder_config' å’Œ 'classifier_config' çš„å­—å…¸ï¼Œæˆ– ClassifierWrapper å¯¹è±¡")
                
                # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                self.model.eval()
                self.model.to(self.device)
                print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (state_dict æ–¹å¼)ï¼Œè®¾å¤‡: {self.device}")
                
            elif self.mode == "jit":
                # JIT æ¨¡å¼ï¼šä½¿ç”¨ torch.jit.load
                self.model = torch.jit.load(str(model_path), map_location=self.device)
                self.model.eval()
                self.model.to(self.device)
                print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ (TorchScript)ï¼Œè®¾å¤‡: {self.device}")
                
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„åŠ è½½æ¨¡å¼: {self.mode}")
                
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
    
    @torch.no_grad()
    def predict(self, graph: dgl.DGLGraph) -> Dict:
        """
        æ‰§è¡Œå•ä¸ªå›¾é¢„æµ‹
        
        Args:
            graph: DGLå¼‚æ„å›¾
            
        Returns:
            é¢„æµ‹ç»“æœå­—å…¸
        """
        start_time = time.time()
        
        # ç§»åŠ¨å›¾åˆ°è®¾å¤‡
        graph = graph.to(self.device)
        
        # æ¨ç†ï¼ˆnative å’Œ jit æ¨¡å¼ç»Ÿä¸€è°ƒç”¨æ–¹å¼ï¼‰
        logits = self.model(graph)
        
        # è®¡ç®—æ¦‚ç‡
        probabilities = torch.softmax(logits, dim=-1)
        
        # è§£æç»“æœ
        predicted_class_id = int(torch.argmax(logits, dim=-1).item())
        confidence = float(torch.max(probabilities).item())
        
        # æ„å»ºæ¦‚ç‡åˆ†å¸ƒ
        prob_dist = {}
        probs_np = probabilities.cpu().numpy().flatten()
        for i, prob in enumerate(probs_np):
            class_name = self.class_mapping.get(i, f"Class_{i}")
            prob_dist[class_name] = float(prob)
        
        total_time = time.time() - start_time
        
        return {
            "predicted_class_id": predicted_class_id,
            "predicted_class": self.class_mapping.get(
                predicted_class_id, 
                f"Class_{predicted_class_id}"
            ),
            "confidence": confidence,
            "probabilities": prob_dist,
            "inference_time": total_time
        }
    
    @torch.no_grad()
    def predict_batch(self, batched_graph: dgl.DGLGraph) -> List[Dict]:
        """
        æ‰¹é‡é¢„æµ‹ï¼ˆæ¥å—å·²batchçš„å›¾ï¼‰
        
        Args:
            batched_graph: å·²batchçš„DGLå›¾
            
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        start_time = time.time()
        
        # ç§»åŠ¨å›¾åˆ°è®¾å¤‡
        batched_graph = batched_graph.to(self.device)
        
        # æ¨ç†
        logits = self.model(batched_graph)
        probabilities = torch.softmax(logits, dim=-1)
        
        # è§£æç»“æœ
        results = []
        probs_np = probabilities.cpu().numpy()
        
        for i in range(len(probs_np)):
            predicted_class_id = int(np.argmax(probs_np[i]))
            confidence = float(np.max(probs_np[i]))
            
            prob_dist = {
                self.class_mapping.get(j, f"Class_{j}"): float(prob)
                for j, prob in enumerate(probs_np[i])
            }
            
            results.append({
                "predicted_class_id": predicted_class_id,
                "predicted_class": self.class_mapping.get(
                    predicted_class_id,
                    f"Class_{predicted_class_id}"
                ),
                "confidence": confidence,
                "probabilities": prob_dist
            })
        
        total_time = time.time() - start_time
        avg_time = total_time / len(results) if results else 0
        
        for result in results:
            result["inference_time"] = avg_time
        
        return results
    
    def get_top_k(self, graph: dgl.DGLGraph, k: int = 3) -> List[Tuple[str, float]]:
        """è·å–Top-Ké¢„æµ‹ç»“æœ"""
        result = self.predict(graph)
        sorted_probs = sorted(
            result["probabilities"].items(),
            key=lambda x: x[1],
            reverse=True
        )
        return sorted_probs[:k]
    
    def is_ready(self) -> bool:
        """æ£€æŸ¥æ¨ç†å¼•æ“æ˜¯å¦å°±ç»ª"""
        return self.model is not None
